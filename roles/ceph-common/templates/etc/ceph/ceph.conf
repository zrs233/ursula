#jinja2: trim_blocks: "true", lstrip_blocks: "true"
# {{ ansible_managed }}

[global]
{% if ceph_common.cephx %}
  auth cluster required = cephx
  auth service required = cephx
  auth client required = cephx
  cephx require signatures = {{ ceph_common.cephx_require_signatures }} # Kernel RBD does NOT support signatures!
  cephx cluster require signatures = {{ ceph_common.cephx_cluster_require_signatures }}
  cephx service require signatures = {{ ceph_common.cephx_service_require_signatures }}
{% else %}
  auth cluster required = none
  auth service required = none
  auth client required = none
  auth supported = none
{% endif %}
  fsid = {{ fsid_file.content | b64decode }}
  max open files = {{ ceph_common.max_open_files }}
  osd pool default pg num = {{ ceph_common.pool_default_pg_num }}
  osd pool default pgp num = {{ ceph_common.pool_default_pgp_num }}
  osd pool default size = {{ ceph_common.pool_default_size }}
  osd pool default min size = {{ ceph_common.pool_default_min_size }}
  osd pool default crush rule = {{ ceph_common.pool_default_crush_rule }}
{% if groups[ceph.osd_group_name][1] is not defined %}
  osd crush chooseleaf type = 0
{% endif %}
{% if ceph_common.disable_in_memory_logs %}
  # Disable in-memory logs
  debug_lockdep = 0/0
  debug_context = 0/0
  debug_crush = 0/0
  debug_buffer = 0/0
  debug_timer = 0/0
  debug_filer = 0/0
  debug_objecter = 0/0
  debug_rados = 0/0
  debug_rbd = 0/0
  debug_journaler = 0/0
  debug_objectcatcher = 0/0
  debug_client = 0/0
  debug_osd = 0/0
  debug_optracker = 0/0
  debug_objclass = 0/0
  debug_filestore = 0/0
  debug_journal = 0/0
  debug_ms = 0/0
  debug_monc = 0/0
  debug_tp = 0/0
  debug_auth = 0/0
  debug_finisher = 0/0
  debug_heartbeatmap = 0/0
  debug_perfcounter = 0/0
  debug_asok = 0/0
  debug_throttle = 0/0
  debug_mon = 0/0
  debug_paxos = 0/0
  debug_rgw = 0/0
{% endif %}
{% if ceph_common.enable_debug_global %}
  debug ms = {{ ceph_common.debug_global_level }}
{% endif %}

[client]
  rbd cache = {{ ceph_common.rbd_cache }}
  rbd cache writethrough until flush = true
  rbd concurrent management ops = {{ ceph_common.rbd_concurrent_management_ops }}
  log file = {{ ceph_common.rbd_client_log_file }} # must be writable by QEMU and allowed by SELinux or AppArmor
  rbd default map options = {{ ceph_common.rbd_default_map_options }}
  rbd default features = {{ ceph_common.rbd_default_features }} # sum features digits
  rbd default format = {{ ceph_common.rbd_default_format }}

[mon]
  mon osd down out interval = {{ ceph_common.mon_osd_down_out_interval }}
  mon osd min down reporters = {{ ceph_common.mon_osd_min_down_reporters }}
  mon clock drift allowed = {{ ceph_common.mon_clock_drift_allowed }}
  mon clock drift warn backoff = {{ ceph_common.mon_clock_drift_warn_backoff }}
  mon osd full ratio = {{ ceph_common.mon_osd_full_ratio }}
  mon osd nearfull ratio = {{ ceph_common.mon_osd_nearfull_ratio }}
  mon osd report timeout = {{ ceph_common.mon_osd_report_timeout }}
  mon pg warn max per osd = {{ ceph_common.mon_pg_warn_max_per_osd }}
  mon osd allow primary affinity = {{ ceph_common.mon_osd_allow_primary_affinity }}

{% if ceph_common.enable_debug_mon %}
  debug mon = {{ ceph_common.debug_mon_level }}
  debug paxos = {{ ceph_common.debug_mon_level }}
  debug auth = {{ ceph_common.debug_mon_level }}
{% endif %}
{% for host in groups[ceph.monitor_group_name] %}
  {% if hostvars[host]['ansible_hostname'] is defined %}
  [mon.{{ hostvars[host]['ansible_hostname'] }}]
    host = {{ hostvars[host]['ansible_hostname'] }}
    mon addr = {{ hostvars[host]['ansible_' + ceph_common.monitor_interface]['ipv4']['address'] }}
  {% endif %}
{% endfor %}

[osd]
  osd mkfs type = {{ ceph_common.osd_mkfs_type }}
  osd mkfs options xfs = {{ ceph_common.osd_mkfs_options_xfs }}
  osd mount options xfs = {{ ceph_common.osd_mount_options_xfs }}
  osd journal size = {{ ceph_common.journal_size }}
  public_network = {{ ceph.public_and_cluster_network }}
  cluster_network = {{ ceph.public_and_cluster_network }}
  osd mon heartbeat interval = {{ ceph_common.osd_mon_heartbeat_interval }}
  osd backfill full ratio = {{ ceph_common.osd_backfill_full_ratio }}
  # Performance tuning
  filestore merge threshold = {{ ceph_common.filestore_merge_threshold }}
  filestore split multiple = {{ ceph_common.filestore_split_multiple }}
  osd op threads = {{ ceph_common.osd_op_threads }}
  filestore op threads = {{ ceph_common.filestore_op_threads }}
  filestore max sync interval = {{ ceph_common.filestore_max_sync_interval }}
  osd max scrubs = {{ ceph_common.osd_max_scrubs }}
  # Recovery tuning
  osd recovery max active = {{ ceph_common.osd_recovery_max_active }}
  osd max backfills = {{ ceph_common.osd_max_backfills }}
  osd recovery op priority = {{ ceph_common.osd_recovery_op_priority }}
  osd recovery max chunk = {{ ceph_common.osd_recovery_max_chunk }}
  osd recovery threads = {{ ceph_common.osd_recovery_threads }}
  osd objectstore = {{ ceph_common.osd_objectstore }}
  osd crush update on start = {{ ceph_common.osd_crush_update_on_start }}
{% if ceph_common.enable_debug_osd %}
  debug osd = {{ ceph_common.debug_osd_level }}
  debug filestore = {{ ceph_common.debug_osd_level }}
  debug journal = {{ ceph_common.debug_osd_level }}
  debug monc = {{ ceph_common.debug_osd_level }}
{% endif %}
  # Deep scrub impact
  osd scrub sleep = {{ ceph_common.osd_scrub_sleep }}
  osd disk thread ioprio class = {{ ceph_common.osd_disk_thread_ioprio_class }}
  osd disk thread ioprio priority = {{ ceph_common.osd_disk_thread_ioprio_priority }}
  osd scrub chunk max = {{ ceph_common.osd_scrub_chunk_max }}
  osd deep scrub stride = {{ ceph_common.osd_deep_scrub_stride }}
