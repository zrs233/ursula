---

ceph_osd:

  ####################
  # OSD CRUSH LOCATION
  ####################

  # The following options will build a ceph.conf with OSD sections
  # Example:
  # [osd.X]
  # osd crush location = "root=location"
  #
  # This works with your inventory file
  # To match the following 'osd_crush_location' option the inventory must look like:
  #
  # [osds]
  # osd0 ceph_crush_root=foo ceph_crush_rack=bar

  crush_location: false
  osd_crush_location: "'root={{ ceph_crush_root }} rack={{ ceph_crush_rack }} host={{ ansible_hostname }}'"

  ##############
  # CEPH OPTIONS
  ##############

  # Devices to be used as OSDs
  # You can pre-provision disks that are not present yet.
  # Ansible will just skip them. Newly added disk will be
  # automatically configured during the next run.
  #

  # USE WITH CAUTION
  # Erase partitions structure and layout of the given devices below prior to prepare them
  zap_devices: false

  # Declare devices
  # All the scenarii inherit from the following device declaration
  #
  # devices:
  #  - /dev/sdb
  #  - /dev/sdc

  # Device discovery is based on the Ansible fact 'ansible_devices'
  # which reports all the devices on a system. If chosen all the disks
  # found will be passed to ceph-disk. You should not be worried on using
  # this option since ceph-disk has a built-in check which looks for empty devices.
  # Thus devices with existing partition tables will not be used.
  # This mode prevents you from filling out the 'devices' variable above.
  #
  osd_auto_discovery: false

  # N journal devices for N OSDs
  # While starting we have 2 options:
  # 1. Pre-allocate all the devices
  # 2. Progressively add new devices

  raw_multi_journal: true
  #raw_journal_devices:
  #  - /dev/sdb
  #  - /dev/sdb
  #  - /dev/sdc
  #  - /dev/sdc
