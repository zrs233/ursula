---
- name: ensure raw_multi_journal was chosen
  fail: msg="please choose an osd scenario"
  when: ceph.osd_group_name is defined and
        not ceph_osd.raw_multi_journal

- name: verify devices have been provided
  fail: msg="please provide devices to your osd scenario"
  when: ceph.osd_group_name is defined and
        ceph_osd.raw_multi_journal and
        ceph_osd.devices is not defined

- name: verify journal devices have been provided
  fail: msg="please provide journal devices to your osd scenario"
  when: ceph.osd_group_name is defined and
        ceph_osd.raw_multi_journal and
        ceph_osd.raw_journal_devices is not defined

- name: install dependencies
  apt: pkg={{ item }}
       state=present
  with_items: ceph_osd.pkgs

- name: register ceph.keyring
  slurp: src=/var/lib/ceph/bootstrap-osd/ceph.keyring
  register: ceph_keyrings
  run_once: true
  delegate_to: "{{ groups[ceph.monitor_group_name][0] }}"

- name: write ceph.keyring
  copy:
    dest: "{{ ceph_keyrings['source'] }}"
    content: "{{ ceph_keyrings['content'] | b64decode }}"

- name: check if 'ceph' partition exists on osd devices
  shell: "parted --script {{ item }} print | egrep -sq '^ 1.*ceph'"
  with_items: ceph_osd.devices
  ignore_errors: true
  changed_when: false
  register: ceph_partition_osds

- name: check if 'ceph' partition exists on journal devices
  shell: "parted --script {{ item }} print | egrep -sq '^ 1.*ceph'"
  with_items: ceph_osd.raw_journal_devices
  ignore_errors: true
  changed_when: false
  register: ceph_partition_journals

# only do the next four tasks when:
#   1. no ceph partitions were found on the disk OR
#   2. user explicitly chooses to destroy disks

- name: erasing partitions and labels from osd devices
  command: sgdisk --zap {{ item.1 }}
  ignore_errors: true
  with_together:
    - ceph_partition_osds.results
    - ceph_osd.devices
  when: item.0.rc != 0 or
        ceph_osd.destroy_devices_no_matter_what

- name: erasing partitions and labels from journal devices
  command: sgdisk --zap {{ item.1 }}
  ignore_errors: true
  with_together:
    - ceph_partition_journals.results
    - ceph_osd.raw_journal_devices
  when: item.0.rc != 0 or
        ceph_osd.destroy_devices_no_matter_what

- name: clear osd devices
  shell: dd if=/dev/zero of={{ item.1 }} bs=1M count=512
  with_together:
    - ceph_partition_osds.results
    - ceph_osd.devices
  when: item.0.rc != 0 or
        ceph_osd.destroy_devices_no_matter_what

- name: clear journal devices
  shell: dd if=/dev/zero of={{ item.1 }} bs=1M count=512
  with_together:
    - ceph_partition_journals.results
    - ceph_osd.raw_journal_devices
  when: item.0.rc != 0 or
        ceph_osd.destroy_devices_no_matter_what

# only do this when no ceph paritions were found on the disk
- name: prepare osd disk(s)
  command: "ceph-disk prepare {{ item.1 }} {{ item.2 }}"
  with_together:
    - ceph_partition_osds.results
    - ceph_osd.devices
    - ceph_osd.raw_journal_devices
  when:
    item.0.rc != 0

# only do this if no ceph partitions were found on the disk
- name: activate osd(s)
  command: ceph-disk activate {{ item.1 }}1
  with_together:
    - ceph_partition_osds.results
    - ceph_osd.devices
  when:
    item.0.rc != 0

- name: start and add that the osd service(s) to the init sequence
  service: name=ceph
           state=started
           enabled=yes

- name: delete default 'rbd' pool
  command: ceph osd pool delete rbd rbd --yes-i-really-really-mean-it
  register: poolout
  changed_when: poolout.stdout | search('removed')
  run_once: true
  delegate_to: "{{ groups[ceph.monitor_group_name][0] }}"

- name: create openstack pool
  ceph_pool:
    pool_name: "{{ ceph_common.pool_name}}"
  register: pool_output
  run_once: true
  delegate_to: "{{ groups[ceph.monitor_group_name][0] }}"
