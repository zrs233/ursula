---
# Wait for a max of 30 minutes
- name: wait for ceph cluster to be in health ok state
  shell: ceph health | egrep -q "HEALTH_OK"
  register: result
  until: result.rc == 0
  retries: 30
  delay: 60
  delegate_to: "{{ groups['ceph_monitors'][0] }}"

# tasks of ceph monitors
- block:
  - name: stop ceph mon
    service: name=ceph-mon-all state=stopped

  - name: correct ceph dir owner
    file: path=/var/lib/ceph state=directory owner=ceph group=ceph recurse=yes
    when: upgrade_ceph2jewel|bool

  - name: start ceph mons
    service: name=ceph-mon-all state=started enabled=yes
  when: "'ceph_monitors' in group_names"

# tasks of ceph osds
- block:
  # Setting noout to prevent cluster from rebalancing after service restart
  - name: set osd noout
    command: ceph osd set noout
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  # Setting osd values to reduce performance degradation of client I/O
  - name: set osd noscrub
    command: ceph osd set noscrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: set osd nodeep scrub
    command: ceph osd set nodeep-scrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: stop ceph osd
    service: name=ceph-osd-all state=stopped

  - include: ceph_owner.yml
    when: upgrade_ceph2jewel|bool

  - name: start ceph osds
    service: name=ceph-osd-all state=started enabled=yes

  # make sure ceph osd services are up before unset noout
  - name: make sure ceph osds are up
    shell: test `ls /var/run/ceph/*.asok |wc -l` -eq {{ ceph.disks|length }}
    register: result
    until: result.rc == 0
    retries: 5
    delay: 5

  # Unset the osd values to make heath check pass
  - name: unset osd noout
    command: ceph osd unset noout
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: unset osd noscrub
    command: ceph osd unset noscrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"

  - name: unset osd nodeep scrub
    command: ceph osd unset nodeep-scrub
    delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: "'ceph_osds' in group_names"

# make sure ceph health ok after update
- name: wait for ceph cluster to be in health ok state
  shell: ceph health | egrep -q "HEALTH_OK"
  register: result
  until: result.rc == 0
  retries: 30
  delay: 60
  delegate_to: "{{ groups['ceph_monitors'][0] }}"
  when: inventory_hostname == groups['ceph_osds'][-1]
