# Play for upgrading from one version of OpenStack to another.
# Heavily flavored for Kilo to Mitaka for now, but useful as a model for
# future upgrades. NOT a rolling upgrade optimized for downtime.
#
# Assumes ml2 networking
---
- name: set the facts
  hosts: all:!vyatta-*
  gather_facts: yes
  tags: always
  tasks:
    - name: set the upgrade fact
      set_fact:
        upgrade: True

    - name: set the git update fact
      set_fact:
        openstack_source:
          git_update: yes

- name: cleanups
  hosts: all:!vyatta-*
  gather_facts: no
  tags: always
  tasks:
    - name: clean out existing apt repos
      file:
        path: /etc/apt/sources.list.d/
        state: absent

    - name: recreate apt sources directory
      file:
        path: /etc/apt/sources.list.d
        state: directory
        owner: root
        group: root
        mode: 0755

- name: upgrade common bits first
  hosts: all:!vyatta-*
  max_fail_percentage: 1
  tags: common
  roles:
    - role: common
  environment: "{{ env_vars|default({}) }}"

- name: upgrade rabbit for security
  hosts: controller
  serial: 1
  max_fail_percentage: 1
  tags: rabbit

  tasks:
    - name: upgrade rabbit
      apt:
        pkg: rabbitmq-server
        state: latest
      notify: restart-rabbit
      register: result
      until: result|succeeded
      retries: 5

  handlers:
    - name: restart-rabbit
      service:
        name: rabbitmq-server
        state: restarted
  environment: "{{ env_vars|default({}) }}"

- name: upgrade client bits
  hosts: all:!vyatta-*
  max_fail_percentage: 1
  tags: client

  pre_tasks:
    - name: remove clients for re-install
      pip:
        name: "{{ item }}"
        state: absent
      with_items:
        - python-openstackclient
        - python-neutronclient
        - python-heatclient
        - python-novaclient
        - python-keystoneclient
      register: result
      until: result|succeeded
      retries: 5

  roles:
    - role: client
  environment: "{{ env_vars|default({}) }}"

- name: stop the restarts
  hosts: all:!vyatta-*
  tags: always
  tasks:
    # work around Ansible 2.0 bug where parent roles do not see vars
    # assigned to child roles
    - name: turn restarts off
      set_fact:
        restart: False

- name: upgrade glance
  hosts: controller
  max_fail_percentage: 1
  tags: glance

  pre_tasks:
    - name: dump glance db
      mysql_db:
        name: glance
        state: dump
        target: /backup/glance-preupgrade.sql
      run_once: True
      tags: dbdump
      delegate_to: "{{ groups['db'][0] }}"

  roles:
    - role: glance
      force_sync: true
      restart: False
      database_create:
        changed: false
  environment: "{{ env_vars|default({}) }}"

- include: playbooks/cinder-upgrade.yml
  when: cinder.enabled|bool

- name: upgrade heat
  hosts: controller
  max_fail_percentage: 1
  tags: heat

  roles:
    - role: heat
      force_sync: true
      restart: False
      database_create:
        changed: false
      when: heat.enabled|bool
  environment: "{{ env_vars|default({}) }}"

# Ceilometer block
- name: stage ceilometer data software
  hosts: compute
  max_fail_percentage: 1
  tags:
    - ceilometer
    - ceilometer-data

  roles:
    - role: ceilometer-data
      restart: False
      when: ceilometer.enabled|bool

    - role: stop-services
      services:
        - ceilometer-agent-compute
      must_exist: False
      when: ceilometer.enabled|bool
  environment: "{{ env_vars|default({}) }}"

- name: start the restarts
  hosts: all:!vyatta-*
  tags: always
  tasks:
    # work around Ansible 2.0 bug where parent roles do not see vars
    # assigned to child roles
    - name: turn restarts on
      set_fact:
        restart: True

- name: stage ceilomter control software and stop services
  hosts: controller
  max_fail_percentage: 1
  tags:
    - ceilometer
    - ceilometer-control

  roles:
    - role: stop-services
      services:
        - ceilometer-agent-central
        - ceilometer-alarm-evaluator
        - ceilometer-alarm-notifier
      must_exist: False
      when: ceilometer.enabled|bool

    - role: ceilometer-control
      when: ceilometer.enabled|bool
  environment: "{{ env_vars|default({}) }}"

- name: start ceilometer data services
  hosts: compute
  max_fail_percentage: 1
  tags:
    - ceilometer
    - ceilometer-data

  tasks:
    - name: start ceilometer data services
      service:
        name: ceilometer-polling
        state: started
      when: ceilometer.enabled|bool

- name: stop the restarts
  hosts: all:!vyatta-*
  tags: always
  tasks:
    # work around Ansible 2.0 bug where parent roles do not see vars
    # assigned to child roles
    - name: turn restarts off
      set_fact:
        restart: False

- name: upgrade keystone
  hosts: controller
  max_fail_percentage: 1
  tags: keystone

  pre_tasks:
    - name: dump keystone db
      mysql_db:
        name: keystone
        state: dump
        target: /backup/keystone-preupgrade.sql
      run_once: True
      tags: dbdump
      delegate_to: "{{ groups['db'][0] }}"

  roles:
    - role: keystone
      force_sync: true
      restart: False
      database_create:
        changed: False
  environment: "{{ env_vars|default({}) }}"

# Nova block
- name: stage nova compute
  hosts: compute
  max_fail_percentage: 1
  tags:
    - nova
    - nova-data

  roles:
    - role: nova-data
      restart: False
      when: ironic.enabled == False

    - role: stop-services
      services:
        - nova-compute
      must_exist: False
      when: ironic.enabled == False
  environment: "{{ env_vars|default({}) }}"

- name: stage nova control and stop services
  hosts: controller
  max_fail_percentage: 1
  tags:
    - nova
    - nova-control

  pre_tasks:
    - name: dump nova db
      mysql_db:
        name: nova
        state: dump
        target: /backup/nova-preupgrade.sql
      run_once: True
      tags: dbdump
      delegate_to: "{{ groups['db'][0] }}"

  roles:
    - role: nova-control
      force_sync: true
      restart: False
      database_create:
        changed: false
  environment: "{{ env_vars|default({}) }}"

- name: start nova compute
  hosts: compute
  max_fail_percentage: 1
  tags:
    - nova
    - nova-data

  tasks:
    - name: start nova data services
      service:
        name: "{{ item }}"
        state: started
      with_items:
        - nova-compute
      when: ironic.enabled == False

# Neutron block
- name: stage neutron core data
  hosts: compute:network
  max_fail_percentage: 1
  tags:
    - neutron
    - neutron-data

  roles:
    - role: neutron-data
      restart: False
  environment: "{{ env_vars|default({}) }}"

- name: stage neutron network
  hosts: network
  max_fail_percentage: 1
  tags:
    - neutron
    - neutron-network

  roles:
    - role: neutron-data-network
      restart: False
  environment: "{{ env_vars|default({}) }}"

- name: upgrade neutron control plane
  hosts: controller
  max_fail_percentage: 1
  tags:
    - neutron
    - neutron-control

  pre_tasks:
    - name: dump neutron db
      mysql_db:
        name: neutron
        state: dump
        target: /backup/neutron-preupgrade.sql
      run_once: True
      tags: dbdump
      delegate_to: "{{ groups['db'][0] }}"

  roles:
    - role: neutron-control
      force_sync: true
      restart: False
      database_create:
        changed: false
  environment: "{{ env_vars|default({}) }}"

- name: restart neutron data service
  hosts: compute:network
  max_fail_percentage: 1
  tags:
    - neutron
    - neutron-data

  tasks:
    - name: restart neutron data service
      service:
        name: neutron-linuxbridge-agent
        state: restarted

- name: restart neutron data network services
  hosts: network
  max_fail_percentage: 1
  tags:
    - neutron
    - neutron-network

  tasks:
    - name: restart neutron data network agent services
      service:
        name: "{{ item }}"
        state: restarted
      with_items:
        - neutron-l3-agent
        - neutron-dhcp-agent
        - neutron-metadata-agent

- name: upgrade swift
  hosts: swiftnode
  any_errors_fatal: true
  tags: swift

  pre_tasks:
    # FIXME Added since we are changing swift init scripts in 3.0.x
    # This will stop services started by "old" scripts. This can be removed
    # after all systems have migrated to 3.0.x
    - name: stop swift services started by swift-init
      command: swift-init stop all
      failed_when: false

    # Ownership needs to be changed for future use, do this at upgrade time
    - name: set swiftops user as owner of ring definition
      file:
        path: /etc/swift/ring_definition.yml
        owner: swiftops
        group: swiftops
      run_once: true
      delegate_to: "{{ groups['swiftnode_primary'][0] }}"

  roles:
    - role: haproxy
      haproxy_type: swift
      tags: ['openstack', 'swift', 'control']

    - role: swift-object
      tags: ['openstack', 'swift', 'data']

    - role: swift-account
      tags: ['openstack', 'swift', 'data']

    - role: swift-container
      tags: ['openstack', 'swift', 'data']

    - role: swift-proxy
      tags: ['openstack', 'swift', 'control']
  environment: "{{ env_vars|default({}) }}"

- name: start the restarts
  hosts: all:!vyatta-*
  tags: always
  tasks:
    # work around Ansible 2.0 bug where parent roles do not see vars
    # assigned to child roles
    - name: turn restarts on
      set_fact:
        restart: True

- name: upgrade horizon
  hosts: controller
  max_fail_percentage: 1
  tags: horizon

  roles:
    - role: horizon
  environment: "{{ env_vars|default({}) }}"

- include: site.yml
